{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports:\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import modules.predict as predict\n",
    "import modules.encoder as encoder\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('./import/TEST_BEFORE_COMPLETION.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#before performing the mapping we want to fill incomplete columns \n",
    "target_columns = ['numberOfFloors_original', 'rooftype_original']\n",
    "selected_predictors = ['class', 'first_year', 'last_year', 'numberOfFloors_original', 'material.value', 'rooftype_original']\n",
    "gdf_new = copy.deepcopy(gdf)\n",
    "for target in target_columns:\n",
    "    filled_column, is_pred_column = predict.fill_missing_values(target, gdf, selected_predictors)\n",
    "    #gdf[target] = filled_column\n",
    "    #put the filled column in the gdf substituting the original one\n",
    "    gdf_new[target] = filled_column\n",
    "    gdf_new[target + '_type'] = is_pred_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gabled', 'flat', 'cupola', 'destroyed building'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify in the mapping which columns should change name. \n",
    "mapping = {\n",
    "    'type': 'class',\n",
    "    'time.estimatedStart.timeMoment.year': 'first_year',    \n",
    "    'time.estimatedEnd.timeMoment.year': 'last_year',  \n",
    "    'numberOfFloors.value':'numberOfFloors_original',\n",
    "    'numberOfFloors.paradata.type':'numberOfFloors_original_type',\n",
    "    'roof.type.value': 'rooftype_original',\n",
    "    'roof.type.paradata.type': 'rooftype_original_type'\n",
    "}\n",
    "# on the left, the name of the field according to the Historical CityJSON extension. On the right, the name of the field in the input geodata.\n",
    "# The fields that are already correctly encoded don't need to be inserted in the mapping\n",
    "# it's not necessary to eliminate non mappable fields, when creating the cityjson they will be ignored\n",
    "\n",
    "# Read the CSV file with the fields to check\n",
    "fields_df = pd.read_csv('../extension/geojson_mapping.csv')\n",
    "# Map the GeoDataFrame to HistoricalCityJSON\n",
    "mapped_gdf = predict.map_gdf_to_historicalcityjson(gdf_new, fields_df, mapping)\n",
    "\n",
    "mapped_gdf['roof.type.value'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf= mapped_gdf\n",
    "for row in gdf.iterrows():\n",
    "    if row[1]['roof.type.value'] == 'gabled' or row[1]['roof.type.value'] == 'slanted':\n",
    "        row[1]['roof.type.value'] = random.choice(['hip','gable'])\n",
    "        gdf.loc[row[0],'roof.type.value'] = row[1]['roof.type.value']\n",
    "    if row[1]['numberOfFloors.value'] == '1-3':\n",
    "        row[1]['numberOfFloors.value'] = random.choice([1,2,3])\n",
    "        gdf.loc[row[0],'numberOfFloors.value'] = row[1]['numberOfFloors.value']\n",
    "    if row[1]['numberOfFloors.value'] == '4+':\n",
    "        row[1]['numberOfFloors.value'] = random.choice([4,5,6])\n",
    "        gdf.loc[row[0],'numberOfFloors.value'] = row[1]['numberOfFloors.value']\n",
    "    if row[1]['numberOfFloors.value'] == '6+':\n",
    "        row[1]['numberOfFloors.value'] = random.choice([6,7,8])\n",
    "        gdf.loc[row[0],'numberOfFloors.value'] = row[1]['numberOfFloors.value']\n",
    "    if row[1]['type'] == 'archway':\n",
    "        row[1]['type'] = 'Archway'\n",
    "        gdf.loc[row[0],'type'] = row[1]['type']\n",
    "\n",
    "mapping_type = {'archway':'Archway'}\n",
    "mapping_rooftype = {\n",
    "    'destroyed building': '',\n",
    "    'cupola':'domed',\n",
    "                    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped_values = encoder.map_column_values(gdf, 'type', mapping_type)\n",
    "df_mapped_values = encoder.map_column_values(df_mapped_values, 'roof.type.value', mapping_rooftype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gable', 'flat', 'hip', 'domed', ''], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapped_values['roof.type.value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe as a geojson\n",
    "df_mapped_values.to_file('./import/TEST_AFTER_COMPLETION.geojson', driver='GeoJSON')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('dhCityModeller')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6045bf2f8d7785ceaee67a8df419762f500f59e1943ace876e6b428e232b0e87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
