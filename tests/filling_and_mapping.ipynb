{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports:\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import modules.predict as predict\n",
    "import modules.encoder as encoder\n",
    "import copy\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('./import/TEST_BEFORE_COMPLETION.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before performing the mapping we want to fill incomplete columns \n",
    "target_columns = ['numberOfFloors_original', 'rooftype_original']\n",
    "selected_predictors = ['class', 'first_year', 'last_year', 'numberOfFloors_original', 'material.value', 'rooftype_original']\n",
    "gdf_new = copy.deepcopy(gdf)\n",
    "for target in target_columns:\n",
    "    filled_column, is_pred_column, comments_col = predict.fill_missing_values(target, gdf, selected_predictors)\n",
    "    #gdf[target] = filled_column\n",
    "    #put the filled column in the gdf substituting the original one\n",
    "    gdf_new[target] = filled_column\n",
    "    gdf_new[target + '_type'] = is_pred_column\n",
    "    gdf_new[target + '_comments'] = comments_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fill missing values should be called also during the life of the model\n",
    "# it should keep track of the used model\n",
    "encoder should consider all the mapping fields\n",
    "encoder should consider ranges defined as an extraroot property of the model\n",
    "extraroot properties should be listed in the csv\n",
    "every field of the extension should be documented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the comments column with the one in the original gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gabled', 'flat', 'cupola', 'destroyed building'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify in the mapping which columns should change name. \n",
    "mapping = {\n",
    "    'type': 'class',\n",
    "    'time.estimatedStart.year': 'first_year',    \n",
    "    'time.estimatedEnd.year': 'last_year',  \n",
    "    'numberOfFloors.value':'numberOfFloors_original',\n",
    "    'numberOfFloors.paradata.type':'numberOfFloors_original_type',\n",
    "    'roof.type.value': 'rooftype_original',\n",
    "    'roof.type.paradata.type': 'rooftype_original_type'\n",
    "}\n",
    "# on the left, the name of the field according to the Historical CityJSON extension. On the right, the name of the field in the input geodata.\n",
    "# The fields that are already correctly encoded don't need to be inserted in the mapping\n",
    "# it's not necessary to eliminate non mappable fields, when creating the cityjson they will be ignored\n",
    "\n",
    "# Map the GeoDataFrame to HistoricalCityJSON\n",
    "mapped_gdf = encoder.map_gdf_fields_to_historicalcityjson(gdf_new, mapping)\n",
    "\n",
    "mapped_gdf['roof.type.value'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf= mapped_gdf\n",
    "for row in gdf.iterrows():\n",
    "    if row[1]['roof.type.value'] == 'gabled' or row[1]['roof.type.value'] == 'slanted':\n",
    "        row[1]['roof.type.value'] = random.choice(['hip','gable'])\n",
    "        gdf.loc[row[0],'roof.type.value'] = row[1]['roof.type.value']\n",
    "    if row[1]['numberOfFloors.value'] == '1-3':\n",
    "        row[1]['numberOfFloors.value'] = random.choice([1,2,3])\n",
    "        gdf.loc[row[0],'numberOfFloors.value'] = row[1]['numberOfFloors.value']\n",
    "    if row[1]['numberOfFloors.value'] == '4+':\n",
    "        row[1]['numberOfFloors.value'] = random.choice([4,5,6])\n",
    "        gdf.loc[row[0],'numberOfFloors.value'] = row[1]['numberOfFloors.value']\n",
    "    if row[1]['numberOfFloors.value'] == '6+':\n",
    "        row[1]['numberOfFloors.value'] = random.choice([6,7,8])\n",
    "        gdf.loc[row[0],'numberOfFloors.value'] = row[1]['numberOfFloors.value']\n",
    "\n",
    "\n",
    "mapping_type = {'archway':'Archway'}\n",
    "mapping_rooftype = {\n",
    "    'destroyed building': '',\n",
    "    'cupola':'domed',\n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped_values = encoder.map_column_values(gdf, 'type', mapping_type)\n",
    "df_mapped_values = encoder.map_column_values(df_mapped_values, 'roof.type.value', mapping_rooftype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hip', 'flat', 'gable', 'domed', ''], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mapped_values['roof.type.value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe as a geojson\n",
    "df_mapped_values.to_file('./import/TEST_AFTER_COMPLETION.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fields</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>height.value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>height.sources.n.name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>height.sources.n.type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>height.sources.n.notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>time.estimatedEnd.sources.n.notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>time.estimatedEnd.paradata.type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>time.estimatedEnd.paradata.author</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>time.estimatedEnd.paradata.date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>time.estimatedEnd.paradata.comments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  fields\n",
       "0                                   type\n",
       "1                           height.value\n",
       "2                  height.sources.n.name\n",
       "3                  height.sources.n.type\n",
       "4                 height.sources.n.notes\n",
       "..                                   ...\n",
       "143    time.estimatedEnd.sources.n.notes\n",
       "144      time.estimatedEnd.paradata.type\n",
       "145    time.estimatedEnd.paradata.author\n",
       "146      time.estimatedEnd.paradata.date\n",
       "147  time.estimatedEnd.paradata.comments\n",
       "\n",
       "[148 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields_df = pd.read_csv('../extension/geojson_mapping.csv')\n",
    "fields_df\n",
    "# we want to iterate over each row and take the value before the . in a list (only unique, since many elements share the first part before the dot)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             fields\n",
      "0                                              type\n",
      "1                                      height.value\n",
      "2                             height.sources.n.name\n",
      "3                             height.sources.n.type\n",
      "4                            height.sources.n.notes\n",
      "..                                              ...\n",
      "143    time.estimatedEnd.timeMoment.sources.n.notes\n",
      "144      time.estimatedEnd.timeMoment.paradata.type\n",
      "145    time.estimatedEnd.timeMoment.paradata.author\n",
      "146      time.estimatedEnd.timeMoment.paradata.date\n",
      "147  time.estimatedEnd.timeMoment.paradata.comments\n",
      "\n",
      "[148 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(fields_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roof.parameters.railingHeight', 'floorHeight', 'roof.parameters.gableSides', 'numberOfFloors', 'roof.parameters.upperFloorThickness', 'time.estimatedStart', 'time.appearances.n', 'roof.parameters.baseFloorThickness', 'height', 'parcels.n', 'roof.type', 'roof.parameters.domePercentBaseRadius', 'time.estimatedEnd', 'roof.parameters.domePercentVertRadius', 'roof.parameters.slope', 'roof.parameters.eavesOverhang', 'roof.parameters.railingWidth', 'material']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "fields_df = pd.read_csv('../extension/geojson_mapping.csv')\n",
    "def extract_unique_values(fields_df):\n",
    "    unique_values = set()\n",
    "    for field in fields_df['fields']:\n",
    "        if '.value' in field:\n",
    "            value = field.split('.value')[0]\n",
    "            unique_values.add(value)\n",
    "        elif '.sources' in field:\n",
    "            value = field.split('.sources')[0]\n",
    "            unique_values.add(value)\n",
    "        elif '.paradata' in field:\n",
    "            value = field.split('.paradata')[0]\n",
    "            unique_values.add(value)\n",
    "\n",
    "    return list(unique_values)\n",
    "\n",
    "unique_values = extract_unique_values(fields_df)\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['floorHeight']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = unique_values[1].split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roof', 'parameters', 'gableSides']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values[2].split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mestimatedEnd\u001b[38;5;241m.\u001b[39mparadata\u001b[38;5;241m.\u001b[39mcomments\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "time.estimatedEnd.paradata.comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('dhCityModeller')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6045bf2f8d7785ceaee67a8df419762f500f59e1943ace876e6b428e232b0e87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
